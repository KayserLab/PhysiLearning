# Version 0.3.5
# global configuration
global:
  machine: 'mela' # 'mela' # 'raven'
  evaluate_after: 0

# job configuration important for training
job:
  nodes: 1
  ntasks: 1
  mem-per-task: 2000
  cpus-per-task: 2
  time: '24:00:00'
  agent_buffer: 0
  recurrent:
    enable: 0
    n_jobs: 2

# Reinforcement learning configuration
learning:
  # Model options
  model:
    name: 'PPO' #'RecurrentPPO' 'PPO' 'QRDQN' 'DQN' 'A2C' 'ACKTR' 'ACER' 'DDPG' 'PPO2' 'SAC' 'TRPO'
    policy: 'MlpPolicy' #'MlpLstmPolicy' # 'MlpPolicy'# 'CnnPolicy' # 'MultiInputPolicy'
    average_steps: 8
    model_kwargs:
      n_steps: 1024
      batch_size: 64
      n_epochs: 10
      verbose: 0
      ent_coef: 1.e-4
      clip_range: 0.001
      learning_rate: 1.e-4
      policy_kwargs:
#        features_extractor_kwargs:
#          cnn_output_dim: 16
        net_arch:
          pi: [128, 128]
          vf: [128, 128]
    total_timesteps: 5.e+6
    save_freq: 'best' # 1.e+6 use string for best reward save, int for every n steps
    model_save_prefix: '20250206_slv_1_1'
    load:
      enable_loading: 0
      last_model: 0
      saved_model_name: './Training/SavedModels/20241206_pretrain_treat_stpo_best_reward'

# Environment configuration
env:
  # general env settings
  type: 'SLvEnv' # 'PcEnv' # 'LvEnv' # 'GridEnv' # 'SLvEnv'
  n_envs: 1
  wrap: True
  wrapper: 'VecFrameStack' # 'DummyVecEnv' # 'VecFrameStack' # 'SubprocVecEnv'
  wrapper_kwargs: #
    n_stack: 64
    channels_order: 'last' # use first for image learning and last for number

  # general simulation settings
  observation_type: 'number' # 'image' # 'number' # 'multiobs' # 'mutant_position'
  action_type: 'discrete' # 'discrete' # 'continuous'
  see_resistance: False
  see_prev_action: True
  max_tumor_size: 2.0 # 2006 #3086 #100000 # 110000 # 4000
  max_time: 720 #122 # 120
  initial_wt: 3584 #39771 # 1539 #2761 # 74291 # 2761 # 8000 - used for meltd atserhii train
  initial_mut: 5 # 4 # 'random' - used for meltd atserhii train
  growth_rate_wt: 0.040 #0.096 #'0.096pm0.001 ' # 0.079 - meltd. 0.116 - current
  growth_rate_mut: 0.134 #0.12 # 0.076 - meltd. 0.23 - current
  death_rate_wt: 0.01
  death_rate_mut: 0.01
  treat_death_rate_wt: 330 #3.59 #0.075 #0.108 #0.118 #2.076 #'2.076pm0.005' # 2.388 - meltd. 2.57 - current
  treat_death_rate_mut: 0.00 # 0.0
  treatment_time_step: 1 #6
  reward_shaping: 'average' # 'dont_treat', 'ttp', 'seven_days_margin', 'less_than_x.y', 'average'
  normalize: True
  normalize_to: 1.0
  image_size: 124 #84 #124
  patient_sampling:
    enable: 0
    type: 'sequential' # 'random' # 'sequential'
    patient_id: [ 113 ] # [1, 4, 55, 80, 93]

  # LV environment settings
  LvEnv:
    carrying_capacity: 18000 #120000 # '48300pm1210'
    competition_wt: 4.78 # 1.732 - fixed treat #1.772 #1.802 - 1.3# 1.795 # 1.323 #2400.0
    competition_mut: 1.0 # 3.408 #3.299 - 1.3 # 3.319 # 0.0 #0.336
    growth_function_flag: 'instant_fixed_treat_with_noise' # 'instant', 'delayed', 'delayed_with_noise'
    image_sampling_type: 'dense' #'mutant_position' # 'dense' # 'random'
    k: 0.57
    t0: 1.12
  SLvEnv:
    carrying_capacity: 18000 #120000 # '48300pm1210'
    competition_wt: 4.78 # 1.732 - fixed treat #1.772 #1.802 - 1.3# 1.795 # 1.323 #2400.0
    competition_mut: 1.0 # 3.408 #3.299 - 1.3 # 3.319 # 0.0 #0.336
    growth_function_flag: 'instant_fixed_treat_with_noise' #'instant_fixed_treat_with_noise' # 'instant', 'delayed', 'delayed_with_noise'
    image_sampling_type: 'mutant_position' #'mutant_position' # 'dense' # 'random'
    mutant_distance_to_front: 207 #70.5 # 92
    cell_volume: 2494 #2144
    growth_layer: 150 # 110
    dimension: 2
    growth_fit: 'quadratic'


# evaluation configuration
eval:
  most_recent: 0
  from_file: 1
  path: './data/GRAPE_important_data/SLV_training/'
  model_prefix: '20250206_slv_1_9'
  step_to_load: '_best_reward'
  save_name: '_at50_agent_slv'
  # evaluation settings
  fixed_AT_protocol: 0
  at_type: 'fixed' #'effective_high_1.20_low_0.80' # 'mtd' # 'fixed' # 'zhang_et_al'# 'no_treatment'
  threshold: 1.0
  num_episodes: 50
  evaluate_on: 'SLvEnv' # 'PcEnv' # 'LvEnv' # 'GridEnv'
  pcdl: False
